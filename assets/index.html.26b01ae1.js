const e={key:"v-a991a318",path:"/post/2020/07/17/transformer/",title:"Transformer",lang:"en-US",frontmatter:{layout:"Post",title:"Transformer",subtitle:"\u8BD5\u56FE\u7406\u4E00\u7406 Transformer",author:"Renovamen",date:"2020-07-17T00:00:00.000Z",headerImage:"/img/in-post/2020-07-17/header.jpg",permalinkPattern:"/post/:year/:month/:day/:slug/",tags:["NLP"]},excerpt:`<p><strong>Attention Is All You Need.</strong> <em>Ashish Vaswani, et al.</em> NIPS 2017. <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py" target="_blank" rel="noopener noreferrer">[Code]</a></p>
`,headers:[{level:2,title:"Position Embedding",slug:"position-embedding",children:[]},{level:2,title:"Encoder",slug:"encoder",children:[{level:3,title:"Muti-Head Self-Attention",slug:"muti-head-self-attention",children:[]},{level:3,title:"Feed-Forward Network",slug:"feed-forward-network",children:[]}]},{level:2,title:"Decoder",slug:"decoder",children:[{level:3,title:"Masked Multi-Head Self-Attention",slug:"masked-multi-head-self-attention",children:[]},{level:3,title:"Multi-head Attention",slug:"multi-head-attention",children:[]},{level:3,title:"Feed-Forward Network",slug:"feed-forward-network-1",children:[]}]},{level:2,title:"Summary",slug:"summary",children:[]},{level:2,title:"Reference",slug:"reference",children:[]}],git:{updatedTime:1646986451e3},readingTime:{minutes:10,words:2259},filePathRelative:"posts/2020-07-17-transformer.md"};export{e as data};
