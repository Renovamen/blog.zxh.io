const e={key:"v-7f60f4e6",path:"/post/2021/08/31/attention-conv/",title:"Attention / Conv \u5927\u9505\u70E9",lang:"en-US",frontmatter:{layout:"Post",title:"Attention / Conv \u5927\u9505\u70E9",subtitle:"Self-Attentions and Convolutions",author:"Renovamen",date:"2021-08-31T00:00:00.000Z",headerImage:"/img/in-post/2021-08-31/header.jpg",permalinkPattern:"/post/:year/:month/:day/:slug/",tags:["Deep Learning"]},excerpt:`<p>\u957F\u671F\u8BB0\u5F55\u548C\u5B9E\u73B0\u770B\u8FC7\u7684\u5404\u79CD\u8BBA\u6587\u91CC\u7684\u81EA\u6CE8\u610F\u529B\u548C\u5377\u79EF\u673A\u5236\uFF0C\u5495\u5495\u5495\uFF0C\u5B9E\u73B0\u5730\u5740\u5728\uFF1A<a href="https://github.com/Renovamen/torchop" target="_blank" rel="noopener noreferrer"><v-icon name="ri-link-m" scale="0.9"/> Github</a></p>
`,headers:[{level:2,title:"Attention",slug:"attention",children:[{level:3,title:"Self-Attenion",slug:"self-attenion",children:[]},{level:3,title:"SAGAN",slug:"sagan",children:[]},{level:3,title:"External Attention",slug:"external-attention",children:[]},{level:3,title:"Fastformer",slug:"fastformer",children:[]},{level:3,title:"HaloNets",slug:"halonets",children:[]},{level:3,title:"Linformer",slug:"linformer",children:[]}]},{level:2,title:"Convolution",slug:"convolution",children:[{level:3,title:"Selective Kernel",slug:"selective-kernel",children:[]},{level:3,title:"Squeeze-and-Excitation",slug:"squeeze-and-excitation",children:[]},{level:3,title:"Involution",slug:"involution",children:[]}]}],git:{updatedTime:1647724771e3},readingTime:{minutes:15,words:3288},filePathRelative:"posts/2021-08-31-attention-conv.md"};export{e as data};
