const e={key:"v-23c8e74f",path:"/post/2020/03/17/papers-reading/",title:"\u4E09\u6708\u5927\u9505\u70E9",lang:"en-US",frontmatter:{layout:"Post",title:"\u4E09\u6708\u5927\u9505\u70E9",subtitle:"Papers Reading: Machine Translation / Text Classification / Image Captioning",author:"Renovamen",date:"2020-03-17T00:00:00.000Z",headerImage:"/img/in-post/2020-03-17/header.jpg",catalog:!0,permalinkPattern:"/post/:year/:month/:day/:slug/",tags:["NLP","CV","Machine Translation","Text Classification","Image Captioning","Image Aesthetic Captioning"]},excerpt:`<p>\u770B\u8FC7\u7684\u5173\u4E8E\u673A\u5668\u7FFB\u8BD1 / \u6587\u672C\u6458\u8981 / \u56FE\u50CF\u63CF\u8FF0\u7684\u8BBA\u6587\u7684\u603B\u7ED3\uFF0C\u5230\u65F6\u5019\u5927\u6982\u4E5F\u53EF\u4EE5\u76F4\u63A5\u590D\u5236\u7C98\u8D34\u8FDB\u6BD5\u4E1A\u8BBA\u6587\u91CC\u3002</p>
`,headers:[{level:2,title:"Machine Translation",slug:"machine-translation",children:[{level:3,title:"Seq2Seq",slug:"seq2seq",children:[{level:4,title:"Encoder",slug:"encoder",children:[]},{level:4,title:"Decoder",slug:"decoder",children:[]}]},{level:3,title:"Seq2Seq + Attention",slug:"seq2seq-attention",children:[{level:4,title:"Encoder",slug:"encoder-1",children:[]},{level:4,title:"Decoder",slug:"decoder-1",children:[]}]},{level:3,title:"Unsupervised NMT",slug:"unsupervised-nmt",children:[]}]},{level:2,title:"Text Classification",slug:"text-classification",children:[{level:3,title:"Hierarchical Attention Network",slug:"hierarchical-attention-network",children:[{level:4,title:"Word Encoder",slug:"word-encoder",children:[]},{level:4,title:"Word Attention",slug:"word-attention",children:[]},{level:4,title:"Sentence Encoder",slug:"sentence-encoder",children:[]},{level:4,title:"Sentence Attention",slug:"sentence-attention",children:[]},{level:4,title:"Document Classification",slug:"document-classification",children:[]}]}]},{level:2,title:"Image Captioning",slug:"image-captioning",children:[{level:3,title:"Show and Tell",slug:"show-and-tell",children:[{level:4,title:"CNN-LSTM",slug:"cnn-lstm",children:[]},{level:4,title:"LSTM",slug:"lstm",children:[{level:5,title:"Training",slug:"training",children:[]},{level:5,title:"Inference",slug:"inference",children:[]}]},{level:4,title:"Experiments",slug:"experiments",children:[]}]},{level:3,title:"Show, Attend and Tell",slug:"show-attend-and-tell",children:[{level:4,title:"CNN",slug:"cnn",children:[]},{level:4,title:"LSTM + Attention",slug:"lstm-attention",children:[]},{level:4,title:"Experiments",slug:"experiments-1",children:[]}]},{level:3,title:"Adaptive Attention",slug:"adaptive-attention",children:[{level:4,title:"CNN",slug:"cnn-1",children:[]},{level:4,title:"Spatial Attention",slug:"spatial-attention",children:[]},{level:4,title:"Adaptive Attention",slug:"adaptive-attention-1",children:[]},{level:4,title:"Experiments",slug:"experiments-2",children:[]}]},{level:3,title:"Self-critical",slug:"self-critical",children:[{level:4,title:"Policy Gradient",slug:"policy-gradient",children:[]},{level:4,title:"SCST",slug:"scst",children:[]},{level:4,title:"Experiments",slug:"experiments-3",children:[]}]}]},{level:2,title:"Image Aesthetic Captioning",slug:"image-aesthetic-captioning",children:[{level:3,title:"Aesthetic Critiques",slug:"aesthetic-critiques",children:[{level:4,title:"Aspect-oriented",slug:"aspect-oriented",children:[]},{level:4,title:"Aspect-fusion",slug:"aspect-fusion",children:[]},{level:4,title:"PCCD",slug:"pccd",children:[]},{level:4,title:"Experiments",slug:"experiments-4",children:[]}]}]}],git:{updatedTime:1643835026e3},readingTime:{minutes:33,words:7172},filePathRelative:"posts/2020-03-17-papers-reading.md"};export{e as data};
