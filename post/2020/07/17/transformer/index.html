<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Transformer | Renovamen (Xiaohan Zou)</title>
    <meta name="generator" content="VuePress 1.7.1">
    <link rel="icon" href="/img/logo.svg">
    <meta name="description" content="Renovamen's blog, powered by VuePress, themed by Gungnir.">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    
    <link rel="preload" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/css/0.styles.852d2dc2.css" as="style"><link rel="preload" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/app.25468d04.js" as="script"><link rel="preload" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/9.c0f557a8.js" as="script"><link rel="preload" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/1.d17bc128.js" as="script"><link rel="preload" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/2.392998c9.js" as="script"><link rel="preload" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/10.89e21fe4.js" as="script"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/11.656a352d.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/12.c4188696.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/13.b727d30a.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/14.bf97155c.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/15.82fa0f1e.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/16.8e914c29.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/17.fc2fc938.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/18.d0367105.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/19.78a9c95b.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/20.0158e44d.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/21.25fdb8fc.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/22.29157f72.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/23.f2fd6163.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/24.75cdc38a.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/25.d3bcb598.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/26.4597b375.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/27.bf1d29e3.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/28.1fe7eb9c.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/29.4cf68351.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/30.7b81fa77.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/31.485dd86d.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/32.728fff59.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/33.2a6ed037.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/4.056e6071.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/5.60f6ea8c.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/6.886b3198.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/7.7ae91a5f.js"><link rel="prefetch" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/8.92572d1f.js">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/css/0.styles.852d2dc2.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container post-container no-sidebar" data-v-2964d8cc><header class="navbar invert" data-v-2964d8cc><a href="/" class="home-link router-link-active"><span class="site-name">$ cd /home/</span></a> <div class="links"><nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link faa-parent animated-hover"><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="faa-wrench v-icon" style="font-size:1.2em;"><g><path d="M532.72 331.41999999999996h-27.4c-2.6 0-4.6 2-4.6 4.6v32h-36.6V189.71999999999997c0-2.6-2-4.6-4.6-4.6h-27.4c-2.6 0-4.6 2-4.6 4.6v32h-36.6v-32c0-2.6-2-4.6-4.6-4.6h-27.4c-2.6 0-4.6 2-4.6 4.6v32h-36.6v-32c0-6-8-4.6-11.7-4.6v-38c8.3-2 17.1-3.4 25.7-3.4 10.9 0 20.9 4.3 31.4 4.3 4.6 0 27.7-1.1 27.7-8v-60c0-2.6-2-4.6-4.6-4.6-5.1 0-15.1 4.3-24 4.3-9.7 0-20.9-4.3-32.6-4.3-8 0-16 1.1-23.7 2.9v-4.9c5.4-2.6 9.1-8.3 9.1-14.3 0-20.7-31.4-20.8-31.4 0 0 6 3.7 11.7 9.1 14.3v111.7c-3.7 0-11.7-1.4-11.7 4.6v32h-36.6v-32c0-2.6-2-4.6-4.6-4.6h-27.4c-2.6 0-4.6 2-4.6 4.6v32H171.51999999999998v-32c0-2.6-2-4.6-4.6-4.6H139.51999999999998c-2.6 0-4.6 2-4.6 4.6v178.3H98.31999999999998v-32c0-2.6-2-4.6-4.6-4.6H66.31999999999998c-2.6 0-4.6 2-4.6 4.6V555.52h182.9v-96c0-72.6 109.7-72.6 109.7 0v96h182.9V336.02c0.1-2.6-1.9-4.6-4.5-4.6z m-288.1-4.5c0 2.6-2 4.6-4.6 4.6h-27.4c-2.6 0-4.6-2-4.6-4.6v-64c0-2.6 2-4.6 4.6-4.6h27.4c2.6 0 4.6 2 4.6 4.6v64z m146.4 0c0 2.6-2 4.6-4.6 4.6h-27.4c-2.6 0-4.6-2-4.6-4.6v-64c0-2.6 2-4.6 4.6-4.6h27.4c2.6 0 4.6 2 4.6 4.6v64z"></path></g></svg>
  Home
</a></div><div class="nav-item"><a href="/about/" class="nav-link faa-parent animated-hover"><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="faa-wrench v-icon" style="font-size:1.2em;"><g><path d="M299.52 267.52c-79.41 0-192 122.76-192 200.25 0 34.9 26.81 55.75 71.74 55.75 48.84 0 81.09-25.08 120.26-25.08 39.51 0 71.85 25.08 120.26 25.08 44.93 0 71.74-20.85 71.74-55.75C491.52 390.28 378.93 267.52 299.52 267.52z m-147.28-12.61c-10.4-34.65-42.44-57.09-71.56-50.13-29.12 6.96-44.29 40.69-33.89 75.34 10.4 34.65 42.44 57.09 71.56 50.13 29.12-6.96 44.29-40.69 33.89-75.34z m84.72-20.78c30.94-8.14 46.42-49.94 34.58-93.36s-46.52-72.01-77.46-63.87-46.42 49.94-34.58 93.36c11.84 43.42 46.53 72.02 77.46 63.87z m281.39-29.34c-29.12-6.96-61.15 15.48-71.56 50.13-10.4 34.65 4.77 68.38 33.89 75.34 29.12 6.96 61.15-15.48 71.56-50.13 10.4-34.65-4.77-68.38-33.89-75.34z m-156.27 29.34c30.94 8.14 65.62-20.45 77.46-63.87 11.84-43.42-3.64-85.21-34.58-93.36s-65.62 20.45-77.46 63.87c-11.84 43.42 3.64 85.22 34.58 93.36z"></path></g></svg>
  About
</a></div><div class="nav-item"><a href="/tags/" class="nav-link faa-parent animated-hover"><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="faa-wrench v-icon" style="font-size:1.2em;"><g><path d="M43.51999999999998 295.638V91.51999999999998C43.51999999999998 65.00999999999998 65.00999999999998 43.51999999999998 91.51999999999998 43.51999999999998h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L337.34299999999996 541.461c-18.745 18.745-49.137 18.745-67.882 0L57.57899999999998 329.579A48 48 0 0 1 43.51999999999998 295.638zM155.51999999999998 107.51999999999998c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"></path></g></svg>
  Tags
</a></div><div class="nav-item"><a href="/links/" class="nav-link faa-parent animated-hover"><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="faa-wrench v-icon" style="font-size:1.2em;"><g><path d="M348.96954 506.10999999999996c7.39157 7.29792 6.18829 20.09661-3.00038 25.00356-77.713 41.80281-176.72559 29.9105-242.34331-35.7082C38.02375999999998 429.80226999999996 26.115999999999982 330.882 67.93380999999998 253.07399999999998c4.89125-9.095 17.68975-10.29834 25.00318-3.00043L209.74872 366.88707999999997l27.39411-27.39452c-0.68759-2.60974-1.594-5.00071-1.594-7.81361a32.00407 32.00407 0 1 1 32.00407 32.00455c-2.79723 0-5.20378-0.89075-7.79786-1.594l-27.40974 27.41015ZM555.4957999999999 346.58732a16.10336 16.10336 0 0 1-16.002 17.00242H507.38031a15.96956 15.96956 0 0 1-15.89265-15.00213C483.98670999999996 219.0692 379.97348 114.05426999999999 250.55077999999997 107.05327999999997a15.84486 15.84486 0 0 1-15.00191-15.90852V59.54651999999999A16.09389 16.09389 0 0 1 252.551 43.544249999999984C415.77491 52.13921999999998 546.9947199999999 183.361 555.4957999999999 346.58732Zm-96.01221-0.29692a16.21093 16.21093 0 0 1-16.11142 17.29934H411.16499999999996a16.06862 16.06862 0 0 1-15.89265-14.70522c-6.90712-77.01094-68.118-138.91037-144.92467-145.22376a15.94 15.94 0 0 1-14.79876-15.89289V155.65393a16.134 16.134 0 0 1 17.29908-16.096C362.97132 148.0591 451.07626999999997 236.16537999999997 459.48359 346.2904Z"></path></g></svg>
  Links
</a></div> <div class="nav-item"><a class="nav-link faa-parent animated-hover" style="cursor: pointer;"><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="faa-wrench v-icon" style="font-size:1.2em;"><g><path d="M548.52 486.21999999999997L448.82 386.52c-4.5-4.5-10.6-7-17-7H415.52c27.6-35.3 44-79.7 44-128C459.52 136.61999999999998 366.41999999999996 43.51999999999998 251.51999999999998 43.51999999999998S43.51999999999998 136.61999999999998 43.51999999999998 251.51999999999998s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6 0.1-34zM251.51999999999998 379.52c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"></path></g></svg>
      Search
    </a></div></nav></div></header> <div class="sidebar-mask" data-v-2964d8cc></div> <aside class="sidebar" data-v-2964d8cc><div class="personal-info-wrapper" data-v-2964d8cc><div class="mobile-hero-avatar" data-v-2964d8cc><img src="/img/avatar.jpeg" alt="hero" data-v-2964d8cc></div> <p class="mobile-heading" data-v-2964d8cc>Renovamen</p> <div class="sns-wrapper" data-v-2964d8cc><a href="https://github.com/Renovamen" target="_blank" rel="noopener noreferrer"><svg aria-hidden="true" width="0" height="0" viewBox="0 0 0 0" focusable="false" class="icon-stack v-icon" style="font-size:1.2em;"><g><!----> <svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="icon-sns v-icon v-inverse" style="font-size:1.2em;"><g><path d="M245.61999999999998 372.21999999999997c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM539.52 321.71999999999997c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2z m-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-0.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3z m-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"></path></g></svg></g></svg></a><a href="https://www.linkedin.com/in/xiaohan-zou-55bba0160" target="_blank" rel="noopener noreferrer"><svg aria-hidden="true" width="0" height="0" viewBox="0 0 0 0" focusable="false" class="icon-stack v-icon" style="font-size:1.2em;"><g><!----> <svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="icon-sns v-icon v-inverse" style="font-size:1.2em;"><g><path d="M175.79999999999998 491.52H82.91999999999999V192.42h92.88zM129.30999999999997 151.61999999999998C99.60999999999999 151.61999999999998 75.51999999999998 127.01999999999998 75.51999999999998 97.31999999999998a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM523.42 491.52h-92.68V345.91999999999996c0-34.7-0.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V491.52h-92.78V192.42h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V491.52z"></path></g></svg></g></svg></a><a href="https://www.facebook.com/renovamen.zou" target="_blank" rel="noopener noreferrer"><svg aria-hidden="true" width="0" height="0" viewBox="0 0 0 0" focusable="false" class="icon-stack v-icon" style="font-size:1.2em;"><g><!----> <svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="icon-sns v-icon v-inverse" style="font-size:1.2em;"><g><path d="M418.65999999999997 331.52l14.22-92.66h-88.91v-60.13c0-25.35 12.42-50.06 52.24-50.06h40.42V49.77999999999998S399.95 43.51999999999998 364.88 43.51999999999998c-73.22 0-121.08 44.38-121.08 124.72v70.62H162.40999999999997V331.52h81.39v224h100.17V331.52z"></path></g></svg></g></svg></a><a href="https://www.twitter.com/renovamen_zxh" target="_blank" rel="noopener noreferrer"><svg aria-hidden="true" width="0" height="0" viewBox="0 0 0 0" focusable="false" class="icon-stack v-icon" style="font-size:1.2em;"><g><!----> <svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="icon-sns v-icon v-inverse" style="font-size:1.2em;"><g><path d="M502.89 195.236c0.325 4.548 0.325 9.097 0.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447 0.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-0.975-84.792-31.188-98.112-72.772 6.498 0.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></g></svg></g></svg></a><a href="https://www.zhihu.com/people/chao-neng-gui-su" target="_blank" rel="noopener noreferrer"><svg aria-hidden="true" width="0" height="0" viewBox="0 0 0 0" focusable="false" class="icon-stack v-icon" style="font-size:1.2em;"><g><!----> <svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 24 24" focusable="false" class="icon-sns v-icon v-inverse" style="font-size:1.2em;"><g><path d="M12.344 17.963l-1.688 1.074-2.131-3.35c-0.44 1.402-1.172 2.665-2.139 3.825-0.402 0.483-0.82 0.918-1.301 1.375-0.155 0.147-0.775 0.717-0.878 0.82l-1.414-1.414c0.139-0.139 0.787-0.735 0.915-0.856 0.43-0.408 0.795-0.79 1.142-1.206 1.266-1.518 2.03-3.21 2.137-5.231H3v-2h4V7h-0.868c-0.689 1.266-1.558 2.222-2.618 2.857L2.486 8.143c1.395-0.838 2.425-2.604 3.038-5.36l1.952 0.434c-0.14 0.633-0.303 1.227-0.489 1.783H11.5v2H9v4h2.5v2H9.185l3.159 4.963z m3.838-0.07L17.298 17H19V7h-4v10h0.736l0.446 0.893zM13 5h8v14h-3l-2.5 2-1-2H13V5z"></path></g></svg></g></svg></a><a href="mailto:renovamenzxh@gmail.com" target="_blank" rel="noopener noreferrer"><svg aria-hidden="true" width="0" height="0" viewBox="0 0 0 0" focusable="false" class="icon-stack v-icon" style="font-size:1.2em;"><g><!----> <svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="icon-sns v-icon v-inverse" style="font-size:1.2em;"><g><path d="M545.8199999999999 234.32c3.9-3.1 9.7-0.2 9.7 4.7V443.52c0 26.5-21.5 48-48 48H91.51999999999998c-26.5 0-48-21.5-48-48V239.11999999999998c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7 0.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM299.52 363.52c23.2 0.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H91.51999999999998C65.01999999999998 107.51999999999998 43.51999999999998 129.01999999999998 43.51999999999998 155.51999999999998v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></g></svg></g></svg></a> <a href="/rss.xml" target="_blank" rel="noopener noreferrer"><svg aria-hidden="true" width="0" height="0" viewBox="0 0 0 0" focusable="false" class="icon-stack v-icon" style="font-size:1.2em;"><g><!----> <svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="icon-sns v-icon v-inverse" style="font-size:1.2em;"><g><path d="M203.60099999999997 459.479c0 35.369-28.672 64.041-64.041 64.041S75.51999999999998 494.84799999999996 75.51999999999998 459.479s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041z m175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C83.17599999999999 219.28499999999997 75.51999999999998 226.62499999999997 75.51999999999998 235.77299999999997v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772 0.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149 0.001 16.489-7.655 15.995-16.79z m144.249 0.288C515.116 273.197 326.985 83.96499999999997 92.02299999999998 75.52999999999997 82.99299999999998 75.20599999999999 75.51999999999998 82.50099999999998 75.51999999999998 91.53599999999997v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465 0.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-0.001 16.329-7.474 16.005-16.504z"></path></g></svg></g></svg></a></div> <hr data-v-2964d8cc></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link faa-parent animated-hover"><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="faa-wrench v-icon" style="font-size:1.2em;"><g><path d="M532.72 331.41999999999996h-27.4c-2.6 0-4.6 2-4.6 4.6v32h-36.6V189.71999999999997c0-2.6-2-4.6-4.6-4.6h-27.4c-2.6 0-4.6 2-4.6 4.6v32h-36.6v-32c0-2.6-2-4.6-4.6-4.6h-27.4c-2.6 0-4.6 2-4.6 4.6v32h-36.6v-32c0-6-8-4.6-11.7-4.6v-38c8.3-2 17.1-3.4 25.7-3.4 10.9 0 20.9 4.3 31.4 4.3 4.6 0 27.7-1.1 27.7-8v-60c0-2.6-2-4.6-4.6-4.6-5.1 0-15.1 4.3-24 4.3-9.7 0-20.9-4.3-32.6-4.3-8 0-16 1.1-23.7 2.9v-4.9c5.4-2.6 9.1-8.3 9.1-14.3 0-20.7-31.4-20.8-31.4 0 0 6 3.7 11.7 9.1 14.3v111.7c-3.7 0-11.7-1.4-11.7 4.6v32h-36.6v-32c0-2.6-2-4.6-4.6-4.6h-27.4c-2.6 0-4.6 2-4.6 4.6v32H171.51999999999998v-32c0-2.6-2-4.6-4.6-4.6H139.51999999999998c-2.6 0-4.6 2-4.6 4.6v178.3H98.31999999999998v-32c0-2.6-2-4.6-4.6-4.6H66.31999999999998c-2.6 0-4.6 2-4.6 4.6V555.52h182.9v-96c0-72.6 109.7-72.6 109.7 0v96h182.9V336.02c0.1-2.6-1.9-4.6-4.5-4.6z m-288.1-4.5c0 2.6-2 4.6-4.6 4.6h-27.4c-2.6 0-4.6-2-4.6-4.6v-64c0-2.6 2-4.6 4.6-4.6h27.4c2.6 0 4.6 2 4.6 4.6v64z m146.4 0c0 2.6-2 4.6-4.6 4.6h-27.4c-2.6 0-4.6-2-4.6-4.6v-64c0-2.6 2-4.6 4.6-4.6h27.4c2.6 0 4.6 2 4.6 4.6v64z"></path></g></svg>
  Home
</a></div><div class="nav-item"><a href="/about/" class="nav-link faa-parent animated-hover"><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="faa-wrench v-icon" style="font-size:1.2em;"><g><path d="M299.52 267.52c-79.41 0-192 122.76-192 200.25 0 34.9 26.81 55.75 71.74 55.75 48.84 0 81.09-25.08 120.26-25.08 39.51 0 71.85 25.08 120.26 25.08 44.93 0 71.74-20.85 71.74-55.75C491.52 390.28 378.93 267.52 299.52 267.52z m-147.28-12.61c-10.4-34.65-42.44-57.09-71.56-50.13-29.12 6.96-44.29 40.69-33.89 75.34 10.4 34.65 42.44 57.09 71.56 50.13 29.12-6.96 44.29-40.69 33.89-75.34z m84.72-20.78c30.94-8.14 46.42-49.94 34.58-93.36s-46.52-72.01-77.46-63.87-46.42 49.94-34.58 93.36c11.84 43.42 46.53 72.02 77.46 63.87z m281.39-29.34c-29.12-6.96-61.15 15.48-71.56 50.13-10.4 34.65 4.77 68.38 33.89 75.34 29.12 6.96 61.15-15.48 71.56-50.13 10.4-34.65-4.77-68.38-33.89-75.34z m-156.27 29.34c30.94 8.14 65.62-20.45 77.46-63.87 11.84-43.42-3.64-85.21-34.58-93.36s-65.62 20.45-77.46 63.87c-11.84 43.42 3.64 85.22 34.58 93.36z"></path></g></svg>
  About
</a></div><div class="nav-item"><a href="/tags/" class="nav-link faa-parent animated-hover"><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="faa-wrench v-icon" style="font-size:1.2em;"><g><path d="M43.51999999999998 295.638V91.51999999999998C43.51999999999998 65.00999999999998 65.00999999999998 43.51999999999998 91.51999999999998 43.51999999999998h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L337.34299999999996 541.461c-18.745 18.745-49.137 18.745-67.882 0L57.57899999999998 329.579A48 48 0 0 1 43.51999999999998 295.638zM155.51999999999998 107.51999999999998c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"></path></g></svg>
  Tags
</a></div><div class="nav-item"><a href="/links/" class="nav-link faa-parent animated-hover"><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="faa-wrench v-icon" style="font-size:1.2em;"><g><path d="M348.96954 506.10999999999996c7.39157 7.29792 6.18829 20.09661-3.00038 25.00356-77.713 41.80281-176.72559 29.9105-242.34331-35.7082C38.02375999999998 429.80226999999996 26.115999999999982 330.882 67.93380999999998 253.07399999999998c4.89125-9.095 17.68975-10.29834 25.00318-3.00043L209.74872 366.88707999999997l27.39411-27.39452c-0.68759-2.60974-1.594-5.00071-1.594-7.81361a32.00407 32.00407 0 1 1 32.00407 32.00455c-2.79723 0-5.20378-0.89075-7.79786-1.594l-27.40974 27.41015ZM555.4957999999999 346.58732a16.10336 16.10336 0 0 1-16.002 17.00242H507.38031a15.96956 15.96956 0 0 1-15.89265-15.00213C483.98670999999996 219.0692 379.97348 114.05426999999999 250.55077999999997 107.05327999999997a15.84486 15.84486 0 0 1-15.00191-15.90852V59.54651999999999A16.09389 16.09389 0 0 1 252.551 43.544249999999984C415.77491 52.13921999999998 546.9947199999999 183.361 555.4957999999999 346.58732Zm-96.01221-0.29692a16.21093 16.21093 0 0 1-16.11142 17.29934H411.16499999999996a16.06862 16.06862 0 0 1-15.89265-14.70522c-6.90712-77.01094-68.118-138.91037-144.92467-145.22376a15.94 15.94 0 0 1-14.79876-15.89289V155.65393a16.134 16.134 0 0 1 17.29908-16.096C362.97132 148.0591 451.07626999999997 236.16537999999997 459.48359 346.2904Z"></path></g></svg>
  Links
</a></div> <div class="nav-item"><a class="nav-link faa-parent animated-hover" style="cursor: pointer;"><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="faa-wrench v-icon" style="font-size:1.2em;"><g><path d="M548.52 486.21999999999997L448.82 386.52c-4.5-4.5-10.6-7-17-7H415.52c27.6-35.3 44-79.7 44-128C459.52 136.61999999999998 366.41999999999996 43.51999999999998 251.51999999999998 43.51999999999998S43.51999999999998 136.61999999999998 43.51999999999998 251.51999999999998s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6 0.1-34zM251.51999999999998 379.52c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"></path></g></svg>
      Search
    </a></div></nav> <!----> </aside> <div class="post-header" data-v-35e85b42> <div class="header-content" data-v-35e85b42><div class="tags" data-v-35e85b42><span class="tag" data-v-35e85b42>NLP</span></div> <h1 class="title" data-v-35e85b42>Transformer</h1> <h3 class="subtitle" data-v-35e85b42>试图理一理 Transformer</h3> <div class="icons" data-v-35e85b42><div class="icon" data-v-35e85b42><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="v-icon" style="font-size:1.2em;" data-v-35e85b42><g><path d="M389.12 347.52c-28.7 0-42.5 16-89.6 16-47.1 0-60.8-16-89.6-16C135.71999999999997 347.52 75.51999999999998 407.71999999999997 75.51999999999998 481.91999999999996V507.52c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48v-25.6c0-74.2-60.2-134.4-134.4-134.4zM475.52 507.52H123.51999999999998v-25.6c0-47.6 38.8-86.4 86.4-86.4 14.6 0 38.3 16 89.6 16 51.7 0 74.9-16 89.6-16 47.6 0 86.4 38.8 86.4 86.4V507.52zM299.52 331.52c79.5 0 144-64.5 144-144S379.02 43.51999999999998 299.52 43.51999999999998 155.51999999999998 108.01999999999998 155.51999999999998 187.51999999999998s64.5 144 144 144z m0-240c52.9 0 96 43.1 96 96s-43.1 96-96 96-96-43.1-96-96 43.1-96 96-96z"></path></g></svg> <span data-v-35e85b42>Renovamen</span></div> <div class="icon" data-v-35e85b42><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="v-icon" style="font-size:1.2em;" data-v-35e85b42><g><path d="M475.52 107.51999999999998h-48V55.51999999999998c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H235.51999999999998V55.51999999999998c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H123.51999999999998C97.01999999999998 107.51999999999998 75.51999999999998 129.01999999999998 75.51999999999998 155.51999999999998v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V155.51999999999998c0-26.5-21.5-48-48-48z m-6 400H129.51999999999998c-3.3 0-6-2.7-6-6V203.51999999999998h352v298c0 3.3-2.7 6-6 6z"></path></g></svg> <span data-v-35e85b42>2020-07-17</span></div> <div class="icon" data-v-35e85b42><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 24 24" focusable="false" class="v-icon" style="font-size:1.2em;" data-v-35e85b42><g><path d="M17.618 5.968l1.453-1.453 1.414 1.414-1.453 1.453a9 9 0 1 1-1.414-1.414zM12 20a7 7 0 1 0 0-14 7 7 0 0 0 0 14zM11 8h2v6h-2V8zM8 1h8v2H8V1z"></path></g></svg> <span data-v-35e85b42>10 min</span></div></div></div></div> <main class="page"><!----> <div class="theme-content content__default"><p><strong>Attention Is All You Need.</strong> <em>Ashish Vaswani, et al.</em> NIPS 2017. <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py" target="_blank" rel="noopener noreferrer">[Code]</a></p> <p>考虑到 RNN 只能单向依次计算，所以存在以下问题：</p> <ul><li><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathnormal">t</span></span></span></span> 时刻的计算依赖与 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t-1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.69841em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> 时刻的计算结果，限制了模型的并行能力</p></li> <li><p>RNN 的长期依赖问题</p></li></ul> <p>于是这篇论文扔掉了 encoder 和 decoder 中的 RNN 结构，完全用 attention 来搞 machine translation：</p> <ul><li><p>没有 RNN 结构，所以有更好的并行能力</p></li> <li><p>attention 机制对全局信息的处理更有效</p></li></ul> <p>Transformer 整体结构如下：</p> <img src="/img/in-post/2020-07-17/transformer.png" width="400px" alt="Transformer"> <h2 id="position-embedding"><a href="#position-embedding" class="header-anchor">#</a> Position Embedding</h2> <p>Transformer 扔掉了 RNN，对输入句子的所有单词都是同时处理的，所以失去了捕捉单词的排序和位置信息的能力。如果不解决词序的问题，那即使把一句话打乱，attention 出来的结果也是一样的，相当于这就只是一个词袋模型。为了解决这个问题，论文引入 position embedding 来对单词的位置信息进行编码。最终的输入词向量 = word embedding + position embedding：</p> <p><img src="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/img/positional-embedding.bbe028d1.png" alt="Positional Embedding"></p> <p class="desc">图片来源：<a href="http://jalammar.github.io/illustrated-transformer#representing-the-order-of-the-sequence-using-positional-encoding" target="_blank">The Illustrated Transformer</a></p> <p>有两种搞到 position embedding 的思路：</p> <ul><li><p>学习出一份 position embedding（<a href="http://proceedings.mlr.press/v70/gehring17a/gehring17a.pdf" target="_blank" rel="noopener noreferrer"><strong>Convolutional Sequence to Sequence Learning</strong></a>. <em>Jonas Gehring et al.</em> ICML 2017.）</p></li> <li><p>直接用不同频率的 sin 和 cos 函数算出来</p></li></ul> <p>经过实验，论文发现这俩方法效果差不多，所以选了第二种方法，因为它有以下好处：</p> <ul><li><p>不需要加额外的训练参数</p></li> <li><p>学习出来的 position embedding 会受到训练集中序列的长度的限制，但三角函数明显不受序列长度的限制，所以能够处理训练集中没见过的序列长度</p></li></ul> <p>具体的位置编码公式为：</p> <p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><mrow><mn>1000</mn><msup><mn>0</mn><mfrac><mrow><mn>2</mn><mi>i</mi></mrow><msub><mi>d</mi><mtext>model</mtext></msub></mfrac></msup></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE(pos, 2i) = \sin(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}})
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">o</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">2</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.11949em;vertical-align:-1.01193em;"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1075599999999999em;"><span style="top:-2.11em;"><span class="pstrut" style="height:3.12193em;"></span><span class="mord"><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1219299999999999em;"><span style="top:-3.5233700000000003em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8550857142857142em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:0em;margin-right:0.1em;"><span class="pstrut" style="height:2.69444em;"></span><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.34963999999999995em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.2255000000000003em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5937428571428571em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.35193em;"><span class="pstrut" style="height:3.12193em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.79893em;"><span class="pstrut" style="height:3.12193em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mord mathnormal">o</span><span class="mord mathnormal">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.01193em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span></p> <p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>=</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><mrow><mn>1000</mn><msup><mn>0</mn><mfrac><mrow><mn>2</mn><mi>i</mi></mrow><msub><mi>d</mi><mtext>model</mtext></msub></mfrac></msup></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE(pos, 2i+1) = \cos(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}})
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">o</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">2</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.11949em;vertical-align:-1.01193em;"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1075599999999999em;"><span style="top:-2.11em;"><span class="pstrut" style="height:3.12193em;"></span><span class="mord"><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1219299999999999em;"><span style="top:-3.5233700000000003em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8550857142857142em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:0em;margin-right:0.1em;"><span class="pstrut" style="height:2.69444em;"></span><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.34963999999999995em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.2255000000000003em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5937428571428571em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.35193em;"><span class="pstrut" style="height:3.12193em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.79893em;"><span class="pstrut" style="height:3.12193em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mord mathnormal">o</span><span class="mord mathnormal">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.01193em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span></p> <p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext>model</mtext></msub></mrow><annotation encoding="application/x-tex">d_{\text{model}}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 为词嵌入维度（论文中为 512），pos 为该单词在序列中的位置，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>i</mi></mrow><annotation encoding="application/x-tex">2i</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord mathnormal">i</span></span></span></span> 为词向量的偶数维度（用第一个公式），<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">2i+1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.74285em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> 指词向量的奇数维度（用第二个公式）。波的频率和偏移对于每个维度是不同的：</p> <p><img src="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/img/wave.5fd3e2a1.png" alt="wave"></p> <p class="desc">图片来源：<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding" target="_blank">The Annotated Transformer</a></p> <p>因为三角函数还有以下特性：</p> <p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>α</mi><mo>+</mo><mi>β</mi><mo stretchy="false">)</mo><mo>=</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>β</mi><mo stretchy="false">)</mo><mo>−</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>β</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\cos(\alpha + \beta) = \cos(\alpha) \cos(\beta) - \sin(\alpha) \sin(\beta)
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mclose">)</span></span></span></span></span></p> <p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>α</mi><mo>+</mo><mi>β</mi><mo stretchy="false">)</mo><mo>=</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>β</mi><mo stretchy="false">)</mo><mo>+</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>β</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\sin(\alpha + \beta) = \sin(\alpha) \cos(\beta) + \cos(\alpha) \sin(\beta)
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mclose">)</span></span></span></span></span></p> <p>所以任意位置的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo>+</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE(pos+k)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">o</span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span> 都能通过 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE(pos)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">o</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span> 线性表达，这为模型捕捉单词之间的相对位置关系提供了非常大的便利。</p> <h2 id="encoder"><a href="#encoder" class="header-anchor">#</a> Encoder</h2> <p>论文中的 encoder 由 N = 6 个相同的 layer 堆叠而成：</p> <img src="/img/in-post/2020-07-17/encoder.png" width="180px" alt="encoder"> <p>每个 layer 由两个 sub-layer 组成，分别为 multi-head self-attention 和 fully connected feed-forward network。</p> <p>并且每个 sub-layer 都加了：</p> <ul><li><p>Residual Connection：解决多层神经网络训练困难的问题，通过将前一层的信息无差的传递到下一层，可以有效的仅关注差异部分</p></li> <li><p>Layer Normalisation：对层的激活值进行归一化，可以加速模型的训练过程，使其更快的收敛</p> <p><a href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank" rel="noopener noreferrer"><strong>Layer Normalization</strong></a>. <em>Jimmy Lei Ba, et al.</em> arXiv 2016.</p></li></ul> <p>也就是输入会先进 LayerNorm，再进 sub-layer，然后加在原始输入上（虽然图上 LayerNorm 似乎在 sub-layer 后面，但<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html#encoder" target="_blank" rel="noopener noreferrer">代码</a>里的确是先进 LayerNorm）。最后 6 个 layer 都跑完之后还要再单独 norm 一次（虽然图上没画但<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html#encoder" target="_blank" rel="noopener noreferrer">代码</a>里写了）。</p> <h3 id="muti-head-self-attention"><a href="#muti-head-self-attention" class="header-anchor">#</a> Muti-Head Self-Attention</h3> <p>attention 可以表示为以下形式：</p> <p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>a</mi><mi>t</mi><mi>t</mi><mi mathvariant="normal">_</mi><mi>o</mi><mi>u</mi><mi>t</mi><mo>=</mo><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">att\_out = \text{Attention}(Q, K, V)
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.9250799999999999em;vertical-align:-0.31em;"></span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">t</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span></span></span></span></span></p> <p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>（value）用来求加权和得到最终的上下文向量，而 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">Q</span></span></span></span>（query）和所有的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span>（key）会被用来计算注意力权重。如在传统的 seq2seq 结构中，它们分别由以下值经过线性变换得到：</p> <ul><li><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">Q</span></span></span></span>：decoder 的当前输入</p></li> <li><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>：encoder 的输出（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>h</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>h</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">h_1, h_2, \dots, h_n</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>）</p></li> <li><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span>：同 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></p></li></ul> <p>而这里是 self-attention，所以 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo separator="true">,</mo><mi>V</mi><mo separator="true">,</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">Q, V, K</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 由同一个值 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span> 经过线性变换得到，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span> 在第一个 layer 为输入的词向量序列，在之后的 layer 则为上一个 layer 的输出。</p> <p>而 multi-head attention 就是通过 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">h=8</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">8</span></span></span></span> 个不同的线性变换得到不同的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo separator="true">,</mo><mi>V</mi><mo separator="true">,</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">Q, V, K</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span>，最后将这 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span></span></span></span> 个 attention 结果拼接起来：</p> <p><img src="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/img/multi-head-self-attention.6ff112a6.png" alt="multi-head sekf-attention"></p> <p class="desc">图片来源：<a href="http://jalammar.github.io/illustrated-transformer#the-beast-with-many-heads" target="_blank">The Illustrated Transformer</a></p> <p>这里的 attention 计算公式为（scaled dot-product）：</p> <p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}}) V
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.448331em;vertical-align:-0.93em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183309999999999em;"><span style="top:-2.25278em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85722em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.81722em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.18278000000000005em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></p> <p>注意：这里跟 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span> 是<strong>矩阵相乘</strong>，不是 element-wise 相乘。</p> <p><img src="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/img/attention.8d27f0e6.png" alt="attention"></p> <p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub><mo>=</mo><msub><mi>d</mi><mtext>model</mtext></msub><mi mathvariant="normal">/</mi><mi>h</mi><mo>=</mo><mn>512</mn><mi mathvariant="normal">/</mi><mn>8</mn><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">d_k = d_{\text{model}} / h = 512 / 8 = 64</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">5</span><span class="mord">1</span><span class="mord">2</span><span class="mord">/</span><span class="mord">8</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">6</span><span class="mord">4</span></span></span></span>。除以 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.18278000000000005em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85722em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.81722em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.18278000000000005em;"><span></span></span></span></span></span></span></span></span> 是因为，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 越大 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1.035771em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span> 就会越大，可能就会将 softmax 函数推入梯度极小的区域，所以要用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.18278000000000005em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85722em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.81722em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.18278000000000005em;"><span></span></span></span></span></span></span></span></span> 对 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1.035771em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span> 进行缩放。</p> <p>图中的 mask 只会在 <a href="#masked-multi-head-self-attention">decoder</a> 中被用到。</p> <h3 id="feed-forward-network"><a href="#feed-forward-network" class="header-anchor">#</a> Feed-Forward Network</h3> <p>第二个 sub-layer 是一个前馈网络：</p> <p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>FFN</mtext><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><msub><mi>W</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo stretchy="false">)</mo><msub><mi>W</mi><mn>2</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\text{FFN} = \max(0, xW_1 + b_1) W_2 + b_2
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">FFN</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">x</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p> <h2 id="decoder"><a href="#decoder" class="header-anchor">#</a> Decoder</h2> <p>encoder-decoder 结构：</p> <p><img src="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/img/encoder-decoder.cfe8e2a7.png" alt="encoder-decoder"></p> <p class="desc">图片来源：<a href="http://jalammar.github.io/illustrated-transformer#the-residuals" target="_blank">The Illustrated Transformer</a></p> <p><a href="http://jalammar.github.io/illustrated-transformer#the-decoder-side" target="_blank" rel="noopener noreferrer">这里</a>还有两个清楚的解释了 encoder 和 decoder 的工作方式的动画。</p> <p>decoder 也由 N = 6 个相同的 layer 堆叠而成，每个 layer 由三个 sub-layer 组成：</p> <img src="/img/in-post/2020-07-17/decoder.png" width="180px" alt="decoder"> <h3 id="masked-multi-head-self-attention"><a href="#masked-multi-head-self-attention" class="header-anchor">#</a> Masked Multi-Head Self-Attention</h3> <p>在训练时，decoder 在预测第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathnormal">i</span></span></span></span> 个位置时不应该看到未来的信息，但 self-attention 机制能让它看到全局信息（标签泄露）。所以会对在 self-attention 的 softmax 层前加 mask，将未来信息屏蔽掉。</p> <p>mask 是一个下三角矩阵，对角线以及对角线左下都是1，其余都是0：</p> <img src="/img/in-post/2020-07-17/mask.png" width="300px" alt="mask"> <p class="desc">mask 矩阵，蓝色部分是 1，白色部分是 0（图片来源：<a href="https://spaces.ac.cn/archives/6933#单向语言模型" target="_blank">从语言模型到 Seq2Seq：Transformer 如戏，全靠 Mask</a>）</p> <p>矩阵的行为当前预测到第几个单词，列为当前允许看到前几个位置的信息。然后 mask=0 的位置上的元素会都被替换为 <code>-inf</code>。</p> <h3 id="multi-head-attention"><a href="#multi-head-attention" class="header-anchor">#</a> Multi-head Attention</h3> <p>即论文 3.2.3 节中的 encoder-decoder attention。它的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">Q</span></span></span></span> 来自于上一位置的 decoder 的输出（第一个 layer）或上一个 decoder layer 的输出（之后的 layer），而 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span> 来自于 encoder 的输出。这让 decoder 的每一个位置都可以看到到输入序列的全局信息。</p> <p>编码可以并行计算，但解码时，因为需要上一时刻的输出当作 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">Q</span></span></span></span>，所以无法并行计算。</p> <h3 id="feed-forward-network-2"><a href="#feed-forward-network-2" class="header-anchor">#</a> Feed-Forward Network</h3> <p>同 encoder。</p> <h2 id="summary"><a href="#summary" class="header-anchor">#</a> Summary</h2> <p>优点：</p> <ul><li><p>相比其他方法，当序列长度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">n</span></span></span></span> 小于词向量维度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">d</span></span></span></span> 时，每层的计算复杂度（complexity per layer）更低：</p> <p><img src="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/img/complexity.41666cb8.png" alt="complexity"></p></li> <li><p>更好的并行性，符合目前的硬件（GPU）环境</p></li> <li><p>更好地处理长时依赖问题：如果要处理一个长度为 n 的序列，CNN 需要增加卷积层数来扩大视野，RNN 需要从 1 到 n 逐个进行计算，而 self-attention 只需要一步矩阵运算就可以</p></li></ul> <p>缺点：</p> <ul><li><p>但同时从上面那张复杂度表里也能看出来，当句子太长时，Transformer <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 的时间复杂度是非常爆炸的。Transformer 能更好地处理长时依赖问题，但这种复杂度又让它没法处理太长的文本，即使是 Bert 的最大长度也只有 512。</p> <p>于是出现了一堆致力于解决这个问题的后续工作，等我摸两天鱼再看看有没有空写这个...</p></li> <li><p>扔掉了 RNN 和 CNN，导致失去了捕捉局部特征的能力</p> <p>不过论文也提到了一个 restricted self-attention（上面那张复杂度表里有），它假设当前词只与前后 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span> 个词有关，因此只在这 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>r</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">2r+1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> 个词上做 attention，复杂度是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mi>r</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(nr)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mclose">)</span></span></span></span>，相当于是在捕捉局部特征。听上去很像卷积窗口？</p></li> <li><p>失去的位置信息非常重要，在词向量中加入 position embedding 这个解决方案依然不够好</p></li> <li><p>非图灵完备（computationally universal）</p> <p><a href="https://openreview.net/pdf?id=HyzdRiR9Y7" target="_blank" rel="noopener noreferrer"><strong>Universal Transformer</strong></a>. <em>Mostafa Dehghani, et al.</em> ICLR 2019.</p></li></ul> <h2 id="reference"><a href="#reference" class="header-anchor">#</a> Reference</h2> <ul><li><p>图解 Transformer：<a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">The Illustrated Transformer</a></p></li> <li><p>连着代码一起讲：<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener noreferrer">The Annotated Transformer</a></p></li> <li><p><a href="https://zhuanlan.zhihu.com/p/44121378" target="_blank" rel="noopener noreferrer">【NLP】Transformer 详解</a></p></li> <li><p><a href="https://zhuanlan.zhihu.com/p/48508221" target="_blank" rel="noopener noreferrer">详解 Transformer（Attention Is All You Need）</a></p></li></ul></div> <footer class="page-edit"><div class="edit-link"><a href="https://github.com/Renovamen/renovamen.github.io/edit/main/blog/posts/2020-07-17-transformer.md" target="_blank" rel="noopener noreferrer"><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="v-icon" style="font-size:1.2em;"><g><path d="M541.42 185.61999999999998l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM327.71999999999997 143.32L65.11999999999998 405.91999999999996 43.91999999999998 527.42c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM167.61999999999998 383.41999999999996c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM131.51999999999998 467.52h48v36.3l-64.5 11.3-31.1-31.1L95.21999999999998 419.52H131.51999999999998v48z"></path></g></svg>
      Edit this page on GitHub
    </a></div> <div class="last-updated"><span class="prefix">Last Updated:</span> <span class="time">12/25/2020, 11:41:06 AM</span></div></footer> <!----> <div class="pager"><a href="/post/2020/07/10/messy-notes-nlp/" class="previous">
        Previous<br> <span>乱七八糟的知识点</span></a> <a href="/post/2020/08/05/meta-learning/" class="next">
        Previous<br> <span>元学习：一种套娃算法</span></a></div> <!----></main> <ul class="catalog-wrapper" style="top:0px !important;" data-v-628ec192><li class="level-2 toc-link-position-embedding" data-v-628ec192>Position Embedding</li><li class="level-2 toc-link-encoder" data-v-628ec192>Encoder</li><li class="level-3 toc-link-muti-head-self-attention" data-v-628ec192>Muti-Head Self-Attention</li><li class="level-3 toc-link-feed-forward-network" data-v-628ec192>Feed-Forward Network</li><li class="level-2 toc-link-decoder" data-v-628ec192>Decoder</li><li class="level-3 toc-link-masked-multi-head-self-attention" data-v-628ec192>Masked Multi-Head Self-Attention</li><li class="level-3 toc-link-multi-head-attention" data-v-628ec192>Multi-head Attention</li><li class="level-3 toc-link-feed-forward-network-2" data-v-628ec192>Feed-Forward Network</li><li class="level-2 toc-link-summary" data-v-628ec192>Summary</li><li class="level-2 toc-link-reference" data-v-628ec192>Reference</li></ul> <div class="search-page" data-v-2964d8cc><svg aria-hidden="true" width="34.56" height="34.56" viewBox="0 0 599.04 599.04" focusable="false" class="v-icon" style="font-size:2.16em;"><g><path d="M282.549 424.996L88.20599999999999 230.652c-9.373-9.373-9.373-24.569 0-33.941l22.667-22.667c9.357-9.357 24.522-9.375 33.901-0.04L299.52 328.025l154.745-154.021c9.379-9.335 24.544-9.317 33.901 0.04l22.667 22.667c9.373 9.373 9.373 24.569 0 33.941L316.491 424.996c-9.373 9.372-24.569 9.372-33.942 0z"></path></g></svg> <div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div></div> <div class="menu-btn-container" data-v-2964d8cc><div class="menu-btn-wrapper"><div class="menu-btn"><div class="menu-btn-icon" style="display:;"><span></span> <span></span> <span></span></div> <div class="menu-text" style="display:none;">0</div> <svg class="menu-svg"><circle cx="50%" cy="50%" r="48%" class="menu-border" style="stroke-dasharray:0% 314.15926%;"></circle></svg></div> <div class="menu-btn-child-wrapper"><a role="button" aria-label="Toggle light" title="Toggle light" class="toggle-mode menu-btn-child"><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="v-icon" style="font-size:1.2em;"><g><path d="M267.52 139.51999999999998l16-32 32-16-32-16-16-32-16 32-32 16 32 16 16 32zM123.51999999999998 203.51999999999998l26.66-53.33L203.51999999999998 123.51999999999998l-53.34-26.67L123.51999999999998 43.51999999999998 96.85999999999999 96.84999999999998 43.51999999999998 123.51999999999998l53.34 26.67L123.51999999999998 203.51999999999998z m352 128l-26.66 53.33L395.52 411.52l53.34 26.67L475.52 491.52l26.66-53.33L555.52 411.52l-53.34-26.67L475.52 331.52z m70.62-193.77L461.28999999999996 52.899999999999984C455.04999999999995 46.63999999999998 446.85999999999996 43.51999999999998 438.66999999999996 43.51999999999998c-8.19 0-16.38 3.12-22.63 9.38L52.899999999999984 416.03999999999996c-12.5 12.5-12.5 32.76 0 45.25l84.85 84.85c6.25 6.25 14.44 9.37 22.62 9.37 8.19 0 16.38-3.12 22.63-9.37l363.14-363.15c12.5-12.48 12.5-32.75 0-45.24zM402.96999999999997 246.98l-50.91-50.91 86.6-86.6 50.91 50.91-86.6 86.6z"></path></g></svg></a> <div class="menu-btn-child"><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="v-icon" style="font-size:1.2em;"><g><path d="M282.549 424.996L88.20599999999999 230.652c-9.373-9.373-9.373-24.569 0-33.941l22.667-22.667c9.357-9.357 24.522-9.375 33.901-0.04L299.52 328.025l154.745-154.021c9.379-9.335 24.544-9.317 33.901 0.04l22.667 22.667c9.373 9.373 9.373 24.569 0 33.941L316.491 424.996c-9.373 9.372-24.569 9.372-33.942 0z"></path></g></svg></div> <div class="menu-btn-child"><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="v-icon" style="font-size:1.2em;"><g><path d="M316.491 174.04399999999998l194.343 194.343c9.373 9.373 9.373 24.569 0 33.941l-22.667 22.667c-9.357 9.357-24.522 9.375-33.901 0.04L299.52 271.015 144.77499999999998 425.036c-9.379 9.335-24.544 9.317-33.901-0.04l-22.667-22.667c-9.373-9.373-9.373-24.569 0-33.941L282.54999999999995 174.045c9.372-9.373 24.568-9.373 33.941-0.001z"></path></g></svg></div> <div class="menu-btn-child menu-toc-btn"><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 599.04 599.04" focusable="false" class="v-icon" style="font-size:1.2em;"><g><path d="M91.51999999999998 91.51999999999998a48 48 0 1 0 48 48 48 48 0 0 0-48-48z m0 160a48 48 0 1 0 48 48 48 48 0 0 0-48-48z m0 160a48 48 0 1 0 48 48 48 48 0 0 0-48-48z m448 16H219.51999999999998a16 16 0 0 0-16 16v32a16 16 0 0 0 16 16h320a16 16 0 0 0 16-16v-32a16 16 0 0 0-16-16z m0-320H219.51999999999998a16 16 0 0 0-16 16v32a16 16 0 0 0 16 16h320a16 16 0 0 0 16-16V123.51999999999998a16 16 0 0 0-16-16z m0 160H219.51999999999998a16 16 0 0 0-16 16v32a16 16 0 0 0 16 16h320a16 16 0 0 0 16-16v-32a16 16 0 0 0-16-16z"></path></g></svg></div> <div class="menu-btn-child menu-btn-sidebar"><svg aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 24 24" focusable="false" class="v-icon" style="font-size:1.2em;"><g><path d="M3 3h18a1 1 0 0 1 1 1v16a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1z m5 2H4v14h4V5z m2 0v14h10V5H10z"></path></g></svg></div></div></div></div> <div class="footer-wrapper footer" data-v-48eb48db data-v-2964d8cc><span data-v-48eb48db>
      &copy; <a href="https://github.com/Renovamen" target="_blank">Renovamen</a> 2018-2020
      <br>
      Powered by <a href="https://vuepress.vuejs.org" target="_blank">VuePress</a> &
      <a href="https://github.com/Renovamen/vuepress-theme-gungnir" target="_blank">Gungnir</a>
    </span></div></div><div class="global-ui"></div></div>
    <script src="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/app.25468d04.js" defer></script><script src="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/9.c0f557a8.js" defer></script><script src="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/1.d17bc128.js" defer></script><script src="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/2.392998c9.js" defer></script><script src="https://cdn.jsdelivr.net/gh/Renovamen/renovamen.github.io@gh-pages/assets/js/10.89e21fe4.js" defer></script>
  </body>
</html>
